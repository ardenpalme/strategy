{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09d76e9-a7e4-4cdf-9da3-cbd33cd45142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.stattools import adfuller, acf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb90746-a6e4-4677-ac95-f390041930b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Fetching and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d726c8c-5b0b-4b5a-9a8a-a0d1685aad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>106.16</td>\n",
       "      <td>107.59</td>\n",
       "      <td>100.69</td>\n",
       "      <td>101.92</td>\n",
       "      <td>1.035264e+06</td>\n",
       "      <td>103.4447</td>\n",
       "      <td>85673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>101.91</td>\n",
       "      <td>105.28</td>\n",
       "      <td>99.29</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.402462e+06</td>\n",
       "      <td>102.5917</td>\n",
       "      <td>96682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>101.70</td>\n",
       "      <td>110.44</td>\n",
       "      <td>101.47</td>\n",
       "      <td>110.11</td>\n",
       "      <td>1.626238e+06</td>\n",
       "      <td>105.8227</td>\n",
       "      <td>108681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>109.90</td>\n",
       "      <td>117.03</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.74</td>\n",
       "      <td>2.735153e+06</td>\n",
       "      <td>111.1942</td>\n",
       "      <td>200857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>106.77</td>\n",
       "      <td>110.04</td>\n",
       "      <td>71.00</td>\n",
       "      <td>98.59</td>\n",
       "      <td>4.241949e+06</td>\n",
       "      <td>99.1378</td>\n",
       "      <td>276745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close        volume      vwap  \\\n",
       "timestamp                                                            \n",
       "2023-12-30  106.16  107.59  100.69  101.92  1.035264e+06  103.4447   \n",
       "2023-12-31  101.91  105.28   99.29  101.84  1.402462e+06  102.5917   \n",
       "2024-01-01  101.70  110.44  101.47  110.11  1.626238e+06  105.8227   \n",
       "2024-01-02  109.90  117.03  106.00  106.74  2.735153e+06  111.1942   \n",
       "2024-01-03  106.77  110.04   71.00   98.59  4.241949e+06   99.1378   \n",
       "\n",
       "            transactions  \n",
       "timestamp                 \n",
       "2023-12-30         85673  \n",
       "2023-12-31         96682  \n",
       "2024-01-01        108681  \n",
       "2024-01-02        200857  \n",
       "2024-01-03        276745  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ohlc = pd.read_csv('data/SOL_1d_data.csv', parse_dates=True, index_col=0)\n",
    "df_ohlc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a84cdd-9b12-4f4a-a465-4c85691a04d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FEDFUNDS  CPIAUCSL\n",
       "2023-12-30      5.33   309.794\n",
       "2023-12-31      5.33   309.794\n",
       "2024-01-01      5.33   309.794\n",
       "2024-01-02      5.33   311.022\n",
       "2024-01-03      5.33   311.022"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_FED = pd.read_csv('data/FED_data.csv', parse_dates=True, index_col=0)\n",
    "df_FED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c717f3e6-1e2e-44d7-aaab-dfe8175b8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>transactions</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>106.16</td>\n",
       "      <td>107.59</td>\n",
       "      <td>100.69</td>\n",
       "      <td>101.92</td>\n",
       "      <td>1.035264e+06</td>\n",
       "      <td>103.4447</td>\n",
       "      <td>85673</td>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>101.91</td>\n",
       "      <td>105.28</td>\n",
       "      <td>99.29</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.402462e+06</td>\n",
       "      <td>102.5917</td>\n",
       "      <td>96682</td>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>101.70</td>\n",
       "      <td>110.44</td>\n",
       "      <td>101.47</td>\n",
       "      <td>110.11</td>\n",
       "      <td>1.626238e+06</td>\n",
       "      <td>105.8227</td>\n",
       "      <td>108681</td>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>109.90</td>\n",
       "      <td>117.03</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.74</td>\n",
       "      <td>2.735153e+06</td>\n",
       "      <td>111.1942</td>\n",
       "      <td>200857</td>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>106.77</td>\n",
       "      <td>110.04</td>\n",
       "      <td>71.00</td>\n",
       "      <td>98.59</td>\n",
       "      <td>4.241949e+06</td>\n",
       "      <td>99.1378</td>\n",
       "      <td>276745</td>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close        volume      vwap  \\\n",
       "2023-12-30  106.16  107.59  100.69  101.92  1.035264e+06  103.4447   \n",
       "2023-12-31  101.91  105.28   99.29  101.84  1.402462e+06  102.5917   \n",
       "2024-01-01  101.70  110.44  101.47  110.11  1.626238e+06  105.8227   \n",
       "2024-01-02  109.90  117.03  106.00  106.74  2.735153e+06  111.1942   \n",
       "2024-01-03  106.77  110.04   71.00   98.59  4.241949e+06   99.1378   \n",
       "\n",
       "            transactions  FEDFUNDS  CPIAUCSL  \n",
       "2023-12-30         85673      5.33   309.794  \n",
       "2023-12-31         96682      5.33   309.794  \n",
       "2024-01-01        108681      5.33   309.794  \n",
       "2024-01-02        200857      5.33   311.022  \n",
       "2024-01-03        276745      5.33   311.022  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_ohlc, df_FED], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9829e4-40a2-4035-98b7-c825c05c688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_ret'] = np.log(df['close']/(df['close'].shift(1)))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4a6195-1934-426e-b3a4-bc5b3505247f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>transactions</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>101.91</td>\n",
       "      <td>105.28</td>\n",
       "      <td>99.29</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.402462e+06</td>\n",
       "      <td>102.5917</td>\n",
       "      <td>96682</td>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "      <td>-0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>101.70</td>\n",
       "      <td>110.44</td>\n",
       "      <td>101.47</td>\n",
       "      <td>110.11</td>\n",
       "      <td>1.626238e+06</td>\n",
       "      <td>105.8227</td>\n",
       "      <td>108681</td>\n",
       "      <td>5.33</td>\n",
       "      <td>309.794</td>\n",
       "      <td>0.078077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>109.90</td>\n",
       "      <td>117.03</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.74</td>\n",
       "      <td>2.735153e+06</td>\n",
       "      <td>111.1942</td>\n",
       "      <td>200857</td>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "      <td>-0.031084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>106.77</td>\n",
       "      <td>110.04</td>\n",
       "      <td>71.00</td>\n",
       "      <td>98.59</td>\n",
       "      <td>4.241949e+06</td>\n",
       "      <td>99.1378</td>\n",
       "      <td>276745</td>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "      <td>-0.079426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>98.57</td>\n",
       "      <td>108.26</td>\n",
       "      <td>96.67</td>\n",
       "      <td>105.01</td>\n",
       "      <td>2.364127e+06</td>\n",
       "      <td>102.6787</td>\n",
       "      <td>156567</td>\n",
       "      <td>5.33</td>\n",
       "      <td>311.022</td>\n",
       "      <td>0.063086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close        volume      vwap  \\\n",
       "2023-12-31  101.91  105.28   99.29  101.84  1.402462e+06  102.5917   \n",
       "2024-01-01  101.70  110.44  101.47  110.11  1.626238e+06  105.8227   \n",
       "2024-01-02  109.90  117.03  106.00  106.74  2.735153e+06  111.1942   \n",
       "2024-01-03  106.77  110.04   71.00   98.59  4.241949e+06   99.1378   \n",
       "2024-01-04   98.57  108.26   96.67  105.01  2.364127e+06  102.6787   \n",
       "\n",
       "            transactions  FEDFUNDS  CPIAUCSL   log_ret  \n",
       "2023-12-31         96682      5.33   309.794 -0.000785  \n",
       "2024-01-01        108681      5.33   309.794  0.078077  \n",
       "2024-01-02        200857      5.33   311.022 -0.031084  \n",
       "2024-01-03        276745      5.33   311.022 -0.079426  \n",
       "2024-01-04        156567      5.33   311.022  0.063086  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d5f526-5dcb-48e6-885a-31893ed67577",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = df.index[int(len(df) * 0.7)]\n",
    "\n",
    "df_train = df.loc[:split_date]\n",
    "df_test  = df.loc[split_date:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261647c5-f3d1-432c-bac2-68ebb109132c",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f793926-2612-47a6-9268-114f81cd030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, n_features, M, hidden_size=32, n_layers=2):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.use_xpu = torch.xpu.is_available()\n",
    "        if self.use_xpu:\n",
    "            self.device = torch.device(\"xpu:0\")\n",
    "            print(f\"Using Intel XPU: {torch.xpu.get_device_name(0)}\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU\")\n",
    "\n",
    "        self.model = nn.GRU(\n",
    "            input_size=n_features, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.M = M\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.model(x)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "        \n",
    "    def _train(self, df, idx, target, num_epochs=100):\n",
    "        if idx == self.M:\n",
    "            self.scaler.fit(df.iloc[idx-self.M:idx].values)\n",
    "\n",
    "        if(os.path.exists(f'snapshots/model{idx-1}.pth')):\n",
    "            model.load_state_dict(torch.load(f'snapshots/model{idx-1}.pth'))\n",
    "\n",
    "        train_data = df.iloc[idx-self.M:idx].values\n",
    "        train_scaled = self.scaler.transform(train_data)\n",
    "        \n",
    "        train_tensor = torch.FloatTensor(train_scaled).unsqueeze(0).to(self.device)\n",
    "        target_tensor = torch.FloatTensor([[target]]).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        self.train()\n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            output = self.forward(train_tensor)\n",
    "            loss = criterion(output, target_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # if (epoch % 10 == 0):\n",
    "            #     print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "        \n",
    "        #print(f\"[{idx}] training loss: {losses[-1]}\")\n",
    "                \n",
    "        torch.save(model.state_dict(), f'snapshots/model{idx}.pth')\n",
    "        return losses[-1]\n",
    "    \n",
    "    def infer(self, df, idx):\n",
    "        # assert(idx > M)\n",
    "        sequence = df.iloc[idx-self.M:idx].values\n",
    "        sequence_scaled = self.scaler.transform(sequence)\n",
    "        sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(self.device)\n",
    "\n",
    "        self.eval\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(sequence_tensor)\n",
    "            output_np = output.cpu().numpy()\n",
    "\n",
    "        return output_np.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bfc2e60-bc4a-4eec-8306-6af9448df540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Intel XPU: Intel(R) Arc(TM) Graphics\n",
      "[40] training loss: 0.00034922821214422584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.009759514592587948"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN(df.shape[1], M=40)\n",
    "model._train(df, 40, df['log_ret'].iloc[41])\n",
    "model.infer(df,41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2956b723-df04-4ff8-b43c-efd71a85b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41] training loss: 8.0833906395128e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.03295724093914032"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._train(df, 41, df['log_ret'].iloc[42])\n",
    "model.infer(df,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ed158d-6b1a-427b-af2e-afd2c20cf00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Intel XPU: Intel(R) Arc(TM) Graphics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a673c23b84c84f8b98d0a7a5aa795557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Generating Signals:', max=541)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] training loss: 0.07578068226575851\n",
      "[101] training loss: 0.038450371474027634\n",
      "[102] training loss: 0.03639080375432968\n",
      "[103] training loss: 0.07694213837385178\n",
      "[104] training loss: 0.05590067431330681\n",
      "[105] training loss: 0.0016767762135714293\n",
      "[106] training loss: 0.052537135779857635\n",
      "[107] training loss: 0.021937023848295212\n",
      "[108] training loss: 0.02727041393518448\n",
      "[109] training loss: 0.0035384611692279577\n",
      "[110] training loss: 0.018208405002951622\n",
      "[111] training loss: 0.0036727413535118103\n",
      "[112] training loss: 0.017217447981238365\n",
      "[113] training loss: 0.0022037923336029053\n",
      "[114] training loss: 0.011560952290892601\n",
      "[115] training loss: 0.014716473408043385\n",
      "[116] training loss: 0.006348779425024986\n",
      "[117] training loss: 0.009402092546224594\n",
      "[118] training loss: 0.002224657218903303\n",
      "[119] training loss: 0.008684643544256687\n",
      "[120] training loss: 0.005004200153052807\n",
      "[121] training loss: 0.022344296798110008\n",
      "[122] training loss: 4.173565230303211e-06\n",
      "[123] training loss: 0.0024642879143357277\n",
      "[124] training loss: 0.0010084296809509397\n",
      "[125] training loss: 0.0030801519751548767\n",
      "[126] training loss: 0.004162175115197897\n",
      "[127] training loss: 3.0057422918616794e-05\n",
      "[128] training loss: 0.005970112048089504\n",
      "[129] training loss: 0.005008096806704998\n",
      "[130] training loss: 0.0018301134696230292\n",
      "[131] training loss: 0.006596921011805534\n",
      "[132] training loss: 0.0011902892729267478\n",
      "[133] training loss: 0.0019600249361246824\n",
      "[134] training loss: 1.8335136701352894e-05\n",
      "[135] training loss: 0.00515147065743804\n",
      "[136] training loss: 0.006074124947190285\n",
      "[137] training loss: 0.0014676646096631885\n",
      "[138] training loss: 0.0008158858399838209\n",
      "[139] training loss: 0.00011287667439319193\n",
      "[140] training loss: 0.0008826081757433712\n",
      "[141] training loss: 0.007606707978993654\n",
      "[142] training loss: 0.0031045295763760805\n",
      "[143] training loss: 8.961881690083828e-07\n",
      "[144] training loss: 2.910642979259137e-05\n",
      "[145] training loss: 0.0016022123163565993\n",
      "[146] training loss: 7.581317913718522e-05\n",
      "[147] training loss: 0.0011873668991029263\n",
      "[148] training loss: 0.001023472286760807\n",
      "[149] training loss: 0.0007525670807808638\n",
      "[150] training loss: 0.00028309671324677765\n",
      "[151] training loss: 0.00035476917400956154\n",
      "[152] training loss: 0.00022541578800883144\n",
      "[153] training loss: 1.051360959536396e-05\n",
      "[154] training loss: 0.0005235076532699168\n",
      "[155] training loss: 4.7069454012671486e-05\n",
      "[156] training loss: 0.001001050928607583\n",
      "[157] training loss: 1.4785569874220528e-05\n",
      "[158] training loss: 0.0007567474385723472\n",
      "[159] training loss: 0.0017449961742386222\n",
      "[160] training loss: 0.0003458332794252783\n",
      "[161] training loss: 0.0008579642162658274\n",
      "[162] training loss: 0.0007737998967058957\n",
      "[163] training loss: 0.004431477282196283\n",
      "[164] training loss: 0.0011218467261642218\n",
      "[165] training loss: 0.004472063854336739\n",
      "[166] training loss: 0.0018867059843614697\n",
      "[167] training loss: 2.4520906663383357e-05\n",
      "[168] training loss: 1.2500693628680892e-05\n",
      "[169] training loss: 0.008043396286666393\n",
      "[170] training loss: 0.0041492776945233345\n",
      "[171] training loss: 0.0009979195892810822\n",
      "[172] training loss: 0.0016034439904615283\n",
      "[173] training loss: 0.0005722203059121966\n",
      "[174] training loss: 0.002051890129223466\n",
      "[175] training loss: 0.005650043021887541\n",
      "[176] training loss: 2.743902768997941e-05\n",
      "[177] training loss: 1.7268101146328263e-05\n",
      "[178] training loss: 0.0011022186372429132\n",
      "[179] training loss: 0.0031732749193906784\n",
      "[180] training loss: 0.01012963056564331\n",
      "[181] training loss: 0.00018454644305165857\n",
      "[182] training loss: 0.0008498906972818077\n",
      "[183] training loss: 0.00039377648499794304\n",
      "[184] training loss: 0.0008370421710424125\n",
      "[185] training loss: 0.011497157625854015\n",
      "[186] training loss: 0.010296637192368507\n",
      "[187] training loss: 0.0009801978012546897\n",
      "[188] training loss: 0.00023295276332646608\n",
      "[189] training loss: 0.01851733587682247\n",
      "[190] training loss: 0.00041516288183629513\n",
      "[191] training loss: 0.001026655430905521\n",
      "[192] training loss: 0.0010586385615170002\n",
      "[193] training loss: 0.00474674254655838\n",
      "[194] training loss: 0.00017865152040030807\n",
      "[195] training loss: 4.5687846750297467e-07\n",
      "[196] training loss: 0.0005784421227872372\n",
      "[197] training loss: 0.0038064129184931517\n",
      "[198] training loss: 3.62419314114959e-06\n",
      "[199] training loss: 0.0005557525437325239\n",
      "[200] training loss: 0.0018253909656777978\n",
      "[201] training loss: 0.00524670397862792\n",
      "[202] training loss: 0.000982463825494051\n",
      "[203] training loss: 0.0042581744492053986\n",
      "[204] training loss: 0.00041774119017645717\n",
      "[205] training loss: 5.172225064598024e-05\n",
      "[206] training loss: 0.0031960122287273407\n",
      "[207] training loss: 0.00012689700815826654\n",
      "[208] training loss: 0.006610737647861242\n",
      "[209] training loss: 5.500351471710019e-05\n",
      "[210] training loss: 0.0002467406156938523\n",
      "[211] training loss: 5.387063595208019e-08\n",
      "[212] training loss: 4.1423027141718194e-07\n",
      "[213] training loss: 0.00040494432323612273\n",
      "[214] training loss: 9.872152304524207e-07\n",
      "[215] training loss: 0.0043594310991466045\n",
      "[216] training loss: 0.0022706007584929466\n",
      "[217] training loss: 0.0007566397543996572\n",
      "[218] training loss: 0.005191733594983816\n",
      "[219] training loss: 0.012433614581823349\n",
      "[220] training loss: 0.0005061726551502943\n",
      "[221] training loss: 0.005969418678432703\n",
      "[222] training loss: 0.007544447667896748\n",
      "[223] training loss: 0.0017736308509483933\n",
      "[224] training loss: 0.00997589249163866\n",
      "[225] training loss: 0.0003945808857679367\n",
      "[226] training loss: 0.0004540572699625045\n",
      "[227] training loss: 0.0015612111892551184\n",
      "[228] training loss: 0.00040977747994475067\n",
      "[229] training loss: 0.0009871649090200663\n",
      "[230] training loss: 0.00022224387794267386\n",
      "[231] training loss: 3.273677793913521e-05\n",
      "[232] training loss: 4.2696028685895726e-05\n",
      "[233] training loss: 0.0002755440946202725\n",
      "[234] training loss: 6.0132198996143416e-05\n",
      "[235] training loss: 6.774211215088144e-05\n",
      "[236] training loss: 0.004804653115570545\n",
      "[237] training loss: 0.001697102445177734\n",
      "[238] training loss: 6.012260928400792e-05\n",
      "[239] training loss: 2.9935330530861393e-05\n",
      "[240] training loss: 0.0014708202797919512\n",
      "[241] training loss: 0.0002201800816692412\n",
      "[242] training loss: 7.31851343971357e-07\n",
      "[243] training loss: 5.2758155106857885e-06\n",
      "[244] training loss: 0.00015809957403689623\n",
      "[245] training loss: 0.0023164351005107164\n",
      "[246] training loss: 0.0017528911121189594\n",
      "[247] training loss: 0.005290240049362183\n",
      "[248] training loss: 0.0007627030136063695\n",
      "[249] training loss: 0.0031750211492180824\n",
      "[250] training loss: 0.0024166752118617296\n",
      "[251] training loss: 0.00012001203140243888\n",
      "[252] training loss: 8.898112355382182e-06\n",
      "[253] training loss: 0.0003223589446861297\n",
      "[254] training loss: 0.00016884836077224463\n",
      "[255] training loss: 0.0008473343914374709\n",
      "[256] training loss: 0.0008015749044716358\n",
      "[257] training loss: 0.0005919299437664449\n",
      "[258] training loss: 5.1627986977109686e-05\n",
      "[259] training loss: 0.0007087279809638858\n",
      "[260] training loss: 0.0003592405410017818\n",
      "[261] training loss: 0.00025480068870820105\n",
      "[262] training loss: 0.0006360289407894015\n",
      "[263] training loss: 0.002766705583781004\n",
      "[264] training loss: 0.00015810398326721042\n",
      "[265] training loss: 0.00010413238487672061\n",
      "[266] training loss: 0.00099353794939816\n",
      "[267] training loss: 6.126859079813585e-05\n",
      "[268] training loss: 0.002752026543021202\n",
      "[269] training loss: 0.0011770824203267694\n",
      "[270] training loss: 0.002409788779914379\n",
      "[271] training loss: 8.915489161154255e-05\n",
      "[272] training loss: 1.1070180335082114e-05\n",
      "[273] training loss: 0.00029155125957913697\n",
      "[274] training loss: 0.0007139702793210745\n",
      "[275] training loss: 0.0008727536769583821\n",
      "[276] training loss: 0.00011778857879107818\n",
      "[277] training loss: 9.833437070483342e-05\n",
      "[278] training loss: 0.0013125053374096751\n",
      "[279] training loss: 0.0005452040932141244\n",
      "[280] training loss: 5.0242990255355835e-05\n",
      "[281] training loss: 0.0009985665092244744\n",
      "[282] training loss: 0.00010857894812943414\n",
      "[283] training loss: 0.0009227751870639622\n",
      "[284] training loss: 1.0638416824804153e-05\n",
      "[285] training loss: 0.0013349811779335141\n",
      "[286] training loss: 3.88186308555305e-05\n",
      "[287] training loss: 7.651913733752735e-07\n",
      "[288] training loss: 0.0027241630014032125\n",
      "[289] training loss: 0.0007034111185930669\n",
      "[290] training loss: 2.1372504761529854e-06\n",
      "[291] training loss: 0.000255556806223467\n",
      "[292] training loss: 0.0012109912931919098\n",
      "[293] training loss: 0.000718917406629771\n",
      "[294] training loss: 0.0014994671801105142\n",
      "[295] training loss: 0.0001817563024815172\n",
      "[296] training loss: 9.076025889953598e-05\n",
      "[297] training loss: 0.0004364408669061959\n",
      "[298] training loss: 0.0012437283294275403\n",
      "[299] training loss: 0.004273013211786747\n",
      "[300] training loss: 0.002159970812499523\n",
      "[301] training loss: 0.0009865298634395003\n",
      "[302] training loss: 2.8406548153725453e-05\n",
      "[303] training loss: 2.4816099539748393e-05\n",
      "[304] training loss: 0.0004141206736676395\n",
      "[305] training loss: 0.00060790847055614\n",
      "[306] training loss: 3.886154900101246e-06\n",
      "[307] training loss: 3.770684270421043e-05\n",
      "[308] training loss: 0.0004058597842231393\n",
      "[309] training loss: 0.0007430452387779951\n",
      "[310] training loss: 0.0019081452628597617\n",
      "[311] training loss: 0.0065215351060032845\n",
      "[312] training loss: 0.00018370877660345286\n",
      "[313] training loss: 2.1956133423373103e-05\n",
      "[314] training loss: 6.543310155393556e-05\n",
      "[315] training loss: 0.0016589926090091467\n",
      "[316] training loss: 0.001455589197576046\n",
      "[317] training loss: 0.002824201947078109\n",
      "[318] training loss: 0.00039088979247026145\n",
      "[319] training loss: 0.0006142868660390377\n",
      "[320] training loss: 0.0012637245235964656\n",
      "[321] training loss: 0.0005554218660108745\n",
      "[322] training loss: 0.0050824470818042755\n",
      "[323] training loss: 0.0005069230101071298\n",
      "[324] training loss: 0.0005674395360983908\n",
      "[325] training loss: 0.00019447207159828395\n",
      "[326] training loss: 0.00491671497002244\n",
      "[327] training loss: 5.374259490054101e-05\n",
      "[328] training loss: 3.5792530184153293e-07\n",
      "[329] training loss: 3.590972937672632e-06\n",
      "[330] training loss: 0.002062896965071559\n",
      "[331] training loss: 2.054247033811407e-06\n",
      "[332] training loss: 0.0010824870551005006\n",
      "[333] training loss: 0.0015810402110219002\n",
      "[334] training loss: 3.9400805690092966e-05\n",
      "[335] training loss: 0.0011206571944057941\n",
      "[336] training loss: 3.685257979668677e-05\n",
      "[337] training loss: 0.0017303824424743652\n",
      "[338] training loss: 0.0011068020248785615\n",
      "[339] training loss: 0.0007444539805874228\n",
      "[340] training loss: 0.0003881035663653165\n",
      "[341] training loss: 1.9658347810036503e-05\n",
      "[342] training loss: 5.139480435900623e-06\n",
      "[343] training loss: 6.570194727828493e-06\n",
      "[344] training loss: 0.0046806358732283115\n",
      "[345] training loss: 5.115304156788625e-05\n",
      "[346] training loss: 0.0033491968642920256\n",
      "[347] training loss: 0.00014635309344157577\n",
      "[348] training loss: 7.915280002634972e-05\n",
      "[349] training loss: 0.00022815640841145068\n",
      "[350] training loss: 0.000395805633161217\n",
      "[351] training loss: 0.0011223459150642157\n",
      "[352] training loss: 0.000773768697399646\n",
      "[353] training loss: 0.004887051414698362\n",
      "[354] training loss: 0.0016106229741126299\n",
      "[355] training loss: 0.00024409790057688951\n",
      "[356] training loss: 0.003079567803069949\n",
      "[357] training loss: 1.8348642697674222e-05\n",
      "[358] training loss: 0.0017789903795346618\n",
      "[359] training loss: 0.00029317860025912523\n",
      "[360] training loss: 8.88506619958207e-05\n",
      "[361] training loss: 0.0015453945379704237\n",
      "[362] training loss: 6.591622513951734e-05\n",
      "[363] training loss: 0.003229095134884119\n",
      "[364] training loss: 0.0009770771721377969\n",
      "[365] training loss: 9.829957707552239e-05\n",
      "[366] training loss: 5.0570637540658936e-05\n",
      "[367] training loss: 0.0007250000489875674\n",
      "[368] training loss: 0.002843088936060667\n",
      "[369] training loss: 0.0007072435109876096\n",
      "[370] training loss: 0.0003494198899716139\n",
      "[371] training loss: 0.00017596755060367286\n",
      "[372] training loss: 0.000520909670740366\n",
      "[373] training loss: 0.004137684125453234\n",
      "[374] training loss: 5.141967994859442e-05\n",
      "[375] training loss: 0.002059576101601124\n",
      "[376] training loss: 0.0004262500151526183\n",
      "[377] training loss: 1.1952246836699487e-07\n",
      "[378] training loss: 9.891165063891094e-06\n",
      "[379] training loss: 0.0006086657522246242\n",
      "[380] training loss: 0.0004134038172196597\n",
      "[381] training loss: 0.005009389016777277\n",
      "[382] training loss: 2.5658239337644773e-06\n",
      "[383] training loss: 0.00040454830741509795\n",
      "[384] training loss: 0.016603408381342888\n",
      "[385] training loss: 0.0015419715782627463\n",
      "[386] training loss: 0.00011229131632717326\n",
      "[387] training loss: 0.0028069387190043926\n",
      "[388] training loss: 0.00021481547446455806\n",
      "[389] training loss: 0.0006674208561889827\n",
      "[390] training loss: 3.855139948427677e-05\n",
      "[391] training loss: 8.060675895649183e-07\n",
      "[392] training loss: 0.0035783678758889437\n",
      "[393] training loss: 0.00021759828086942434\n",
      "[394] training loss: 0.0005708761163987219\n",
      "[395] training loss: 3.128500247839838e-05\n",
      "[396] training loss: 0.0007900705677457154\n",
      "[397] training loss: 0.001361922244541347\n",
      "[398] training loss: 0.004374745301902294\n",
      "[399] training loss: 0.0004745215701404959\n",
      "[400] training loss: 0.003594698617234826\n",
      "[401] training loss: 0.0015394561924040318\n",
      "[402] training loss: 0.000900572573300451\n",
      "[403] training loss: 5.910936670261435e-05\n",
      "[404] training loss: 0.0003872607194352895\n",
      "[405] training loss: 0.0007525738328695297\n",
      "[406] training loss: 2.245311225124169e-05\n",
      "[407] training loss: 1.2406411769916303e-05\n",
      "[408] training loss: 0.0001016929090837948\n",
      "[409] training loss: 1.4105999071034603e-06\n",
      "[410] training loss: 6.895019760122523e-05\n",
      "[411] training loss: 0.0004840596520807594\n",
      "[412] training loss: 0.0006303125410340726\n",
      "[413] training loss: 0.0005153377423994243\n",
      "[414] training loss: 0.0015954871196299791\n",
      "[415] training loss: 0.0007845585932955146\n",
      "[416] training loss: 4.1475814214209095e-05\n",
      "[417] training loss: 0.0010925380047410727\n",
      "[418] training loss: 0.0018652582075446844\n",
      "[419] training loss: 0.00028984269010834396\n",
      "[420] training loss: 0.00047647050814703107\n",
      "[421] training loss: 0.017150791361927986\n",
      "[422] training loss: 0.0014089931501075625\n",
      "[423] training loss: 0.002644370775669813\n",
      "[424] training loss: 0.0004002852365374565\n",
      "[425] training loss: 0.00316865392960608\n",
      "[426] training loss: 0.001853507710620761\n",
      "[427] training loss: 0.029899679124355316\n",
      "[428] training loss: 0.050115860998630524\n",
      "[429] training loss: 0.0015422936994582415\n",
      "[430] training loss: 0.00031298951944336295\n",
      "[431] training loss: 0.0001175562574644573\n",
      "[432] training loss: 0.00023100388352759182\n",
      "[433] training loss: 8.241606701631099e-05\n",
      "[434] training loss: 0.0032597954850643873\n",
      "[435] training loss: 0.0019488793332129717\n",
      "[436] training loss: 0.002295025158673525\n",
      "[437] training loss: 0.00011773761798394844\n",
      "[438] training loss: 0.001107901567593217\n",
      "[439] training loss: 0.0035321838222444057\n",
      "[440] training loss: 1.3675085028808098e-05\n",
      "[441] training loss: 0.0044499668292701244\n",
      "[442] training loss: 0.0003308603772893548\n",
      "[443] training loss: 0.0002754994493443519\n",
      "[444] training loss: 0.003564658109098673\n",
      "[445] training loss: 0.003946409095078707\n",
      "[446] training loss: 6.129102985141799e-05\n",
      "[447] training loss: 2.4959090296761133e-06\n",
      "[448] training loss: 0.0008705757791176438\n",
      "[449] training loss: 0.00179895362816751\n",
      "[450] training loss: 4.742435976368142e-06\n",
      "[451] training loss: 0.0019282959401607513\n",
      "[452] training loss: 0.00016699974366929382\n",
      "[453] training loss: 0.0020758977625519037\n",
      "[454] training loss: 0.0002333376178285107\n",
      "[455] training loss: 0.00021595550060737878\n",
      "[456] training loss: 5.974718078505248e-06\n",
      "[457] training loss: 0.00016252649948000908\n",
      "[458] training loss: 0.0038983908016234636\n",
      "[459] training loss: 3.791256676777266e-05\n",
      "[460] training loss: 0.0015625391388311982\n",
      "[461] training loss: 0.00039777980418875813\n",
      "[462] training loss: 0.008354585617780685\n",
      "[463] training loss: 0.0006829201010987163\n",
      "[464] training loss: 5.881736342416843e-06\n",
      "[465] training loss: 0.010869531892240047\n",
      "[466] training loss: 0.0028975731693208218\n",
      "[467] training loss: 0.004245261196047068\n",
      "[468] training loss: 0.0033772075548768044\n",
      "[469] training loss: 0.0015279368963092566\n",
      "[470] training loss: 1.1550260751391761e-05\n",
      "[471] training loss: 0.0006349558825604618\n",
      "[472] training loss: 0.000991207780316472\n",
      "[473] training loss: 0.00019850415992550552\n",
      "[474] training loss: 0.00018371650367043912\n",
      "[475] training loss: 0.000937310978770256\n",
      "[476] training loss: 0.00042209174716845155\n",
      "[477] training loss: 0.00010864943033084273\n",
      "[478] training loss: 0.004062274936586618\n",
      "[479] training loss: 5.209559094510041e-05\n",
      "[480] training loss: 2.122324258380104e-05\n",
      "[481] training loss: 0.00017486169235780835\n",
      "[482] training loss: 6.014548853272572e-05\n",
      "[483] training loss: 1.0454714072238858e-07\n",
      "[484] training loss: 2.6026500563602895e-05\n",
      "[485] training loss: 8.349724521394819e-05\n",
      "[486] training loss: 5.785182656836696e-05\n",
      "[487] training loss: 0.0002684958162717521\n",
      "[488] training loss: 0.000345854350598529\n",
      "[489] training loss: 1.721807348076254e-05\n",
      "[490] training loss: 8.772359433351085e-05\n",
      "[491] training loss: 0.00039137134444899857\n",
      "[492] training loss: 2.691559757295181e-07\n",
      "[493] training loss: 2.0367615150007623e-07\n",
      "[494] training loss: 0.006202538963407278\n",
      "[495] training loss: 9.187145042233169e-05\n",
      "[496] training loss: 1.424985566700343e-05\n",
      "[497] training loss: 0.0011107196332886815\n",
      "[498] training loss: 6.298218067968264e-05\n",
      "[499] training loss: 0.001627827063202858\n",
      "[500] training loss: 0.001542287296615541\n",
      "[501] training loss: 0.0006695816409774125\n",
      "[502] training loss: 3.6471985367825255e-05\n",
      "[503] training loss: 1.8585500583867542e-05\n",
      "[504] training loss: 0.0017996628303080797\n",
      "[505] training loss: 0.0009848595364019275\n",
      "[506] training loss: 0.0001932880113599822\n",
      "[507] training loss: 0.0005722756613977253\n",
      "[508] training loss: 0.0003828044282272458\n",
      "[509] training loss: 0.0011707585072144866\n",
      "[510] training loss: 7.67780511523597e-05\n",
      "[511] training loss: 1.1862714927701745e-05\n",
      "[512] training loss: 5.404293688116013e-07\n",
      "[513] training loss: 0.0001451379939680919\n",
      "[514] training loss: 0.0003902086755260825\n",
      "[515] training loss: 0.0004204873985145241\n",
      "[516] training loss: 0.0016459012404084206\n",
      "[517] training loss: 0.00018507077766116709\n",
      "[518] training loss: 0.00013199614477343857\n",
      "[519] training loss: 1.7111824490712024e-06\n",
      "[520] training loss: 3.8463571399915963e-05\n",
      "[521] training loss: 9.0079425717704e-05\n",
      "[522] training loss: 0.001677082502283156\n",
      "[523] training loss: 0.0006738822557963431\n",
      "[524] training loss: 0.00012043072638334706\n",
      "[525] training loss: 0.000122370314784348\n",
      "[526] training loss: 0.0014196987031027675\n",
      "[527] training loss: 1.461949523218209e-05\n",
      "[528] training loss: 0.0010996420169249177\n",
      "[529] training loss: 0.002134424401447177\n",
      "[530] training loss: 0.0001107438511098735\n",
      "[531] training loss: 0.0003160240885335952\n",
      "[532] training loss: 0.0025311470963060856\n",
      "[533] training loss: 0.00023820757633075118\n",
      "[534] training loss: 0.00023899314692243934\n",
      "[535] training loss: 8.037615771172568e-06\n",
      "[536] training loss: 3.0027840693946928e-05\n",
      "[537] training loss: 0.0011000699596479535\n",
      "[538] training loss: 0.0002692035341169685\n",
      "[539] training loss: 7.956226181704551e-05\n",
      "[540] training loss: 0.005530858878046274\n",
      "[541] training loss: 0.00014631098019890487\n",
      "[542] training loss: 0.0004779430746566504\n",
      "[543] training loss: 0.0005059186951257288\n",
      "[544] training loss: 0.0005228287191130221\n",
      "[545] training loss: 0.0017232605023309588\n",
      "[546] training loss: 6.924267381691607e-07\n",
      "[547] training loss: 4.752107088279445e-06\n",
      "[548] training loss: 0.002318428596481681\n",
      "[549] training loss: 0.0009539336315356195\n",
      "[550] training loss: 1.032833279168699e-05\n",
      "[551] training loss: 0.0005414144252426922\n",
      "[552] training loss: 3.4221702662762254e-05\n",
      "[553] training loss: 0.0006444288301281631\n",
      "[554] training loss: 0.0003202992375008762\n",
      "[555] training loss: 0.00016830963431857526\n",
      "[556] training loss: 0.0004117195785511285\n",
      "[557] training loss: 0.0003883864847011864\n",
      "[558] training loss: 0.0005642015603370965\n",
      "[559] training loss: 0.00039572411333210766\n",
      "[560] training loss: 9.210370080836583e-06\n",
      "[561] training loss: 3.733153789653443e-05\n",
      "[562] training loss: 1.9118380805593915e-05\n",
      "[563] training loss: 0.0013346158666536212\n",
      "[564] training loss: 3.583473153412342e-05\n",
      "[565] training loss: 1.605490797373932e-05\n",
      "[566] training loss: 7.997093052836135e-05\n",
      "[567] training loss: 0.0003656067419797182\n",
      "[568] training loss: 0.0025216948706656694\n",
      "[569] training loss: 9.761849651113153e-05\n",
      "[570] training loss: 0.007067721802741289\n",
      "[571] training loss: 0.0001823313214117661\n",
      "[572] training loss: 0.0009567938977852464\n",
      "[573] training loss: 9.819610568229109e-06\n",
      "[574] training loss: 0.0006014476530253887\n",
      "[575] training loss: 0.0002959440171252936\n",
      "[576] training loss: 9.219591561304696e-07\n",
      "[577] training loss: 5.373201929614879e-05\n",
      "[578] training loss: 0.00019808957586064935\n",
      "[579] training loss: 0.0007265429594554007\n",
      "[580] training loss: 1.3400965144683141e-05\n",
      "[581] training loss: 0.0010198303498327732\n",
      "[582] training loss: 0.0015359794488176703\n",
      "[583] training loss: 0.0008663201588205993\n",
      "[584] training loss: 0.00048055945080704987\n",
      "[585] training loss: 0.0007748364005237818\n",
      "[586] training loss: 1.4726204426551703e-05\n",
      "[587] training loss: 2.843666516127996e-05\n",
      "[588] training loss: 1.2398589205986355e-05\n",
      "[589] training loss: 0.0013671068008989096\n",
      "[590] training loss: 0.004503882490098476\n",
      "[591] training loss: 0.0002221585309598595\n",
      "[592] training loss: 0.002856962149962783\n",
      "[593] training loss: 0.0004987537395209074\n",
      "[594] training loss: 0.000303604087093845\n",
      "[595] training loss: 1.3473012586473487e-05\n",
      "[596] training loss: 0.0009854789823293686\n",
      "[597] training loss: 0.0003531049587763846\n",
      "[598] training loss: 0.003483714535832405\n",
      "[599] training loss: 0.0016007199883460999\n",
      "[600] training loss: 0.006089518778026104\n",
      "[601] training loss: 4.8758734919829294e-05\n",
      "[602] training loss: 0.00011319584882585332\n",
      "[603] training loss: 0.006108143832534552\n",
      "[604] training loss: 0.0025530988350510597\n",
      "[605] training loss: 0.00042897812090814114\n",
      "[606] training loss: 0.0005650579114444554\n",
      "[607] training loss: 0.001717999461106956\n",
      "[608] training loss: 0.0001433581783203408\n",
      "[609] training loss: 6.562848739122273e-06\n",
      "[610] training loss: 5.990105364617193e-06\n",
      "[611] training loss: 0.002124690217897296\n",
      "[612] training loss: 0.00038121125544421375\n",
      "[613] training loss: 0.0001997829822357744\n",
      "[614] training loss: 0.00022653424821328372\n",
      "[615] training loss: 0.00035603364813141525\n",
      "[616] training loss: 0.0016147471033036709\n",
      "[617] training loss: 0.0014434305485337973\n",
      "[618] training loss: 0.0009433180675841868\n",
      "[619] training loss: 0.0107824532315135\n",
      "[620] training loss: 1.7771327520677005e-06\n",
      "[621] training loss: 0.008761364035308361\n",
      "[622] training loss: 0.0006191376596689224\n",
      "[623] training loss: 0.0010733944363892078\n",
      "[624] training loss: 0.0007193585624918342\n",
      "[625] training loss: 0.0003386560420040041\n",
      "[626] training loss: 0.00010057524195872247\n",
      "[627] training loss: 0.00104808178730309\n",
      "[628] training loss: 9.253828466171399e-05\n",
      "[629] training loss: 0.0002386244887020439\n",
      "[630] training loss: 0.00011733696010196581\n",
      "[631] training loss: 0.00027563030016608536\n",
      "[632] training loss: 0.0023585855960845947\n",
      "[633] training loss: 3.866286078846315e-06\n",
      "[634] training loss: 1.4456545613938943e-06\n",
      "[635] training loss: 0.0006929569062776864\n",
      "[636] training loss: 8.461836114292964e-05\n",
      "[637] training loss: 0.0003646264085546136\n",
      "[638] training loss: 2.5120971258729696e-06\n",
      "[639] training loss: 0.0011719281319528818\n",
      "[640] training loss: 0.000208766883588396\n"
     ]
    }
   ],
   "source": [
    "win_sz = 100\n",
    "model = NN(df.shape[1], M=win_sz)\n",
    "\n",
    "prog_bar = IntProgress(min=0, max=df.shape[0]-win_sz-1, description='Generating Signals:')\n",
    "display(prog_bar)\n",
    "\n",
    "pred_losses = np.zeros(df.shape[0])\n",
    "predictions = np.zeros(df.shape[0])\n",
    "for idx in range(win_sz,df.shape[0]-1):\n",
    "    target = df['log_ret'].iloc[idx]\n",
    "    \n",
    "    model._train(df, idx, target, num_epochs=20)\n",
    "    predictions[idx] = model.infer(df,idx+1)\n",
    "    pred_losses[idx] = abs(model.infer(df,idx+1) - target)\n",
    "    prog_bar.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d584aa1-34c9-4d64-b51b-398ca6322406",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "ax.set_title(\"NN-Predicted Log Return MSE\")\n",
    "ax.plot(pred_losses[win_sz:-win_sz-1]**2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10783c16-cada-48c4-8f5b-ed7243948a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(predictions)\n",
    "ser.index = df.index\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "ax.set_title(\"NN Incr. Prediction Results\")\n",
    "ax.plot(ser[win_sz:], label=\"NN\")\n",
    "ax.plot(df['log_ret'][win_sz:], label=\"log ret\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bc636-be16-4c84-8766-836d54908bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
